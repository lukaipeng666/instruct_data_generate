#!/usr/bin/env python3
"""
åˆ†å¸ƒå¼æ•°æ®ç”Ÿæˆç®¡é“
å°†æ ·æœ¬æ•°æ®åˆ†é…ç»™å¤šä¸ªæœ¬åœ°æ¨¡å‹æœåŠ¡åŒæ—¶å¤„ç†ï¼Œæœ€å¤§åŒ–æ•°æ®ç”Ÿæˆé€Ÿåº¦
"""

import json
import os
import asyncio
import argparse
import time
import math
from pathlib import Path
from typing import List, Dict, Any
import logging
from develop.single_gen import main_process

# é…ç½®æ—¥å¿—
os.makedirs('log', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('log/pipeline_generation.log', mode='w'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class PipelineDataGenerator:
    def __init__(self, services: List[str], model: str = "/data/models/Qwen3-32B"):
        """
        åˆå§‹åŒ–åˆ†å¸ƒå¼æ•°æ®ç”Ÿæˆå™¨
        
        Args:
            services: APIæœåŠ¡åœ°å€åˆ—è¡¨ï¼Œå¦‚ ["http://localhost:6466/v1", ...]
            model: æ¨¡å‹åç§°
        """
        self.services = services
        self.model = model
        self.service_count = len(services)
        
    def split_samples(self, input_file: str, output_dir: str) -> List[str]:
        """å°†è¾“å…¥æ ·æœ¬æ–‡ä»¶åˆ†å‰²æˆå¤šä¸ªå­æ–‡ä»¶"""
        
        logger.info(f"è¯»å–æ ·æœ¬æ–‡ä»¶: {input_file}")
        
        # è¯»å–æ‰€æœ‰æ ·æœ¬
        samples = []
        with open(input_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    data = json.loads(line.strip())
                    samples.append(data)
                except json.JSONDecodeError as e:
                    logger.warning(f"ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
        
        total_samples = len(samples)
        logger.info(f"æ€»æ ·æœ¬æ•°: {total_samples}")
        
        # è®¡ç®—æ¯ä¸ªæœåŠ¡å¤„ç†çš„æ ·æœ¬æ•°
        samples_per_service = math.ceil(total_samples / self.service_count)
        logger.info(f"æ¯ä¸ªæœåŠ¡å¤„ç†çº¦: {samples_per_service} ä¸ªæ ·æœ¬")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ†å‰²æ•°æ®å¹¶ä¿å­˜
        split_files = []
        for i in range(self.service_count):
            start_idx = i * samples_per_service
            end_idx = min((i + 1) * samples_per_service, total_samples)
            
            if start_idx >= total_samples:
                break
                
            service_samples = samples[start_idx:end_idx]
            split_file = output_path / f"samples_{i+1}_of_{self.service_count}.jsonl"
            
            with open(split_file, 'w', encoding='utf-8') as f:
                for sample in service_samples:
                    json_line = json.dumps(sample, ensure_ascii=False)
                    f.write(json_line + '\n')
            
            split_files.append(str(split_file))
            logger.info(f"åˆ†ç‰‡ {i+1}: {len(service_samples)} ä¸ªæ ·æœ¬ -> {split_file}")
        
        return split_files
    
    async def process_single_service(self, service_idx: int, api_base: str, input_file: str, 
                                   output_file: str, batch_size: int = 5, 
                                   max_concurrent: int = 5, min_score: int = 8,
                                   task_type: str = "entity_extraction",
                                   variants_per_sample: int = 3,
                                   sample_retry_times: int = 3,
                                   model: str = "/data/models/Qwen3-32B",
                                   retry_times: int = 3,
                                   special_prompt: str = "",
                                   directions: list = ["ä¿¡ç”¨å¡å¹´è´¹"]) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæœåŠ¡çš„ä»»åŠ¡"""
        
        logger.info(f"ğŸš€ æœåŠ¡ {service_idx + 1} å¼€å§‹ç”Ÿæˆæ•°æ®: {api_base}")
        start_time = time.time()

        parameter = {
            "input_file": input_file,
            "output_file": output_file, 
            "api_base": api_base, 
            "model": model, 
            "batch_size": batch_size, 
            "max_concurrent": max_concurrent, 
            "retry_times": sample_retry_times, 
            "min_score": min_score, 
            "task_type": task_type, 
            "variants_per_sample": variants_per_sample, 
            "sample_retry_times": sample_retry_times,
            "retry_times":retry_times,
            "special_prompt":special_prompt,
            "directions": directions
        }
        
        try:
            process = await main_process(**parameter)
            
            end_time = time.time()
            duration = end_time - start_time
            
            # æ£€æŸ¥ç»“æœ
            if process["status"] == "Sucessed":
                if os.path.exists(output_file):
                    with open(output_file, 'r') as f:
                        output_count = sum(1 for _ in f)
                    
                    logger.info(f"âœ… æœåŠ¡ {service_idx + 1} å®Œæˆ! è€—æ—¶: {duration:.1f}ç§’, ç”Ÿæˆæ•°æ®: {output_count}æ¡")
                    
                    return {
                        'service_idx': service_idx,
                        'success': True,
                        'duration': duration,
                        'output_file': output_file,
                        'output_count': output_count
                    }
                else:
                    logger.error(f"âŒ æœåŠ¡ {service_idx + 1} æ²¡æœ‰ç”Ÿæˆè¾“å‡ºæ–‡ä»¶")
                    return {'service_idx': service_idx, 'success': False, 'error': 'no_output'}
            else:
                logger.error(f"âŒ æœåŠ¡ {service_idx + 1} å¤„ç†å¤±è´¥ï¼Œè¿”å›ç : {process.returncode}")
                return {'service_idx': service_idx, 'success': False, 'error': f'exit_code_{process.returncode}'}
                
        except Exception as e:
            logger.error(f"âŒ æœåŠ¡ {service_idx + 1} å¼‚å¸¸: {e}")
            return {'service_idx': service_idx, 'success': False, 'error': str(e)}
    
    def merge_outputs(self, output_files: List[str], final_output: str) -> Dict[str, Any]:
        """åˆå¹¶æ‰€æœ‰è¾“å‡ºæ–‡ä»¶"""
        
        logger.info(f"ğŸ“¦ å¼€å§‹åˆå¹¶è¾“å‡ºæ–‡ä»¶åˆ°: {final_output}")
        
        total_records = 0
        successful_files = []
        
        # åˆ›å»ºæœ€ç»ˆè¾“å‡ºæ–‡ä»¶
        with open(final_output, 'w', encoding='utf-8') as final_f:
            for i, output_file in enumerate(output_files):
                if os.path.exists(output_file):
                    file_records = 0
                    with open(output_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            line = line.strip()
                            if line:
                                final_f.write(line + '\n')
                                file_records += 1
                                total_records += 1
                    
                    successful_files.append(output_file)
                    logger.info(f"âœ… åˆå¹¶æ–‡ä»¶ {i+1}: {file_records} æ¡è®°å½•")
                    
                    # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                    os.remove(output_file)
                    logger.info(f"ğŸ—‘ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {output_file}")
                else:
                    logger.warning(f"âš ï¸  è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {output_file}")
        
        result = {
            'total_records': total_records,
            'successful_files': len(successful_files),
            'failed_files': len(output_files) - len(successful_files)
        }
        
        logger.info(f"ğŸ“Š åˆå¹¶å®Œæˆ: {total_records} æ¡è®°å½•")
        return result
    
    def cleanup_split_files(self, split_files: List[str]):
        """æ¸…ç†åˆ†å‰²çš„ä¸´æ—¶æ–‡ä»¶"""
        for split_file in split_files:
            try:
                if os.path.exists(split_file):
                    os.remove(split_file)
                    logger.info(f"ğŸ—‘ï¸  æ¸…ç†åˆ†å‰²æ–‡ä»¶: {split_file}")
            except Exception as e:
                logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {split_file}: {e}")
    
    async def generate_data(self, input_file: str, output_file: str, 
                          batch_size: int = 5, max_concurrent: int = 5,
                          min_score: int = 8, task_type: str = "entity_extraction",
                          variants_per_sample: int = 3, sample_retry_times: int = 3,
                          data_rounds: int = 3, model: str = "/data/models/Qwen3-32B",
                          retry_times: int = 3, special_prompt: str = "",
                          directions: list = ["ä¿¡ç”¨å¡å¹´è´¹"]):
        """ç”Ÿæˆæ•°æ®ï¼Œä½¿ç”¨å¤šä¸ªæœåŠ¡å¹¶è¡Œå¤„ç†ï¼Œæ”¯æŒå¤šè½®æ•°æ®ä½¿ç”¨"""
        
        logger.info("ğŸš€ å¼€å§‹åˆ†å¸ƒå¼æ•°æ®ç”Ÿæˆ")
        logger.info(f"ä½¿ç”¨ {self.service_count} ä¸ªæœåŠ¡:")
        for i, service in enumerate(self.services):
            logger.info(f"  æœåŠ¡ {i+1}: {service}")
        logger.info(f"æ•°æ®ä½¿ç”¨è½®æ¬¡: {data_rounds} è½®")
        
        total_start_time = time.time()
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = Path(output_file).parent / "temp_splits"
        
        # å­˜å‚¨æ‰€æœ‰è½®æ¬¡çš„ç»“æœ
        all_round_results = []
        total_generated_count = 0
        
        # å¤šè½®æ•°æ®å¤„ç†
        for round_num in range(data_rounds):
            logger.info(f"\nğŸ”„ ç¬¬ {round_num + 1}/{data_rounds} è½®æ•°æ®ç”Ÿæˆ")
            
            # 1. åˆ†å‰²æ ·æœ¬æ•°æ®
            logger.info("ğŸ“Š æ ·æœ¬åˆ†å‰²")
            round_dir = temp_dir / f"round_{round_num + 1}"
            split_files = self.split_samples(input_file, str(round_dir))
            
            # 2. å¹¶è¡Œå¤„ç†
            logger.info(f"âš¡ å¹¶è¡Œç”Ÿæˆ ({self.service_count} ä¸ªæœåŠ¡)")
            
            # åˆ›å»ºä»»åŠ¡
            tasks = []
            output_files = []
            
            for i, (service, split_file) in enumerate(zip(self.services, split_files)):
                output_split = str(round_dir / f"generated_{i+1}_of_{self.service_count}.jsonl")
                output_files.append(output_split)
                
                task = self.process_single_service(
                    service_idx=i,
                    api_base=service,
                    input_file=split_file,
                    output_file=output_split,
                    batch_size=batch_size,
                    max_concurrent=max_concurrent,
                    min_score=min_score,
                    task_type=task_type,
                    variants_per_sample=variants_per_sample,
                    sample_retry_times=sample_retry_times,
                    model=model,
                    retry_times=retry_times,
                    special_prompt=special_prompt,
                    directions=directions
                )
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # 3. ç»Ÿè®¡æœ¬è½®ç»“æœ
            logger.info("ğŸ“ˆ æœ¬è½®ç»“æœç»Ÿè®¡")
            successful_count = 0
            round_output_count = 0
            
            for result in results:
                if isinstance(result, dict) and result.get('success'):
                    successful_count += 1
                    round_output_count += result.get('output_count', 0)
                    logger.info(f"âœ… æœåŠ¡ {result['service_idx'] + 1}: {result['output_count']} æ¡, {result['duration']:.1f}ç§’")
                else:
                    if isinstance(result, Exception):
                        logger.error(f"âŒ ä»»åŠ¡å¼‚å¸¸: {result}")
                    else:
                        logger.error(f"âŒ æœåŠ¡å¤±è´¥: {result}")
            
            total_generated_count += round_output_count
            all_round_results.extend(output_files)
            
            logger.info(f"ç¬¬ {round_num + 1} è½®å®Œæˆ: ç”Ÿæˆ {round_output_count} æ¡æ•°æ®")
            
            # 4. æ¸…ç†æœ¬è½®åˆ†å‰²æ–‡ä»¶
            self.cleanup_split_files(split_files)
        
        # 5. åˆå¹¶æ‰€æœ‰è½®æ¬¡çš„è¾“å‡º
        logger.info(f"\nğŸ“¦ åˆå¹¶æ‰€æœ‰ {data_rounds} è½®çš„è¾“å‡ºæ–‡ä»¶")
        merge_result = self.merge_outputs(all_round_results, output_file)
        
        # 6. æ¸…ç†æ‰€æœ‰ä¸´æ—¶ç›®å½•
        logger.info(f"\nğŸ—‘ï¸  æ¸…ç†ä¸´æ—¶ç›®å½•")
        for round_num in range(data_rounds):
            round_dir = temp_dir / f"round_{round_num + 1}"
            try:
                if round_dir.exists():
                    round_dir.rmdir()
                    logger.info(f"ğŸ—‘ï¸  æ¸…ç†è½®æ¬¡ç›®å½•: {round_dir}")
            except:
                pass
        
        try:
            temp_dir.rmdir()
            logger.info(f"ğŸ—‘ï¸  æ¸…ç†ä¸»ä¸´æ—¶ç›®å½•: {temp_dir}")
        except:
            pass
        
        # 7. æ€»ç»“
        total_end_time = time.time()
        total_duration = total_end_time - total_start_time
        
        logger.info(f"\nğŸ‰ åˆ†å¸ƒå¼å¤šè½®æ•°æ®ç”Ÿæˆå®Œæˆ!")
        logger.info(f"=" * 70)
        logger.info(f"æ€»è€—æ—¶: {total_duration:.1f} ç§’")
        logger.info(f"æ•°æ®è½®æ¬¡: {data_rounds} è½®")
        logger.info(f"ä½¿ç”¨æœåŠ¡: {self.service_count} ä¸ª")
        logger.info(f"ç”Ÿæˆæ•°æ®: {merge_result['total_records']} æ¡")
        logger.info(f"å¹³å‡æ¯è½®: {merge_result['total_records']/data_rounds:.1f} æ¡")
        logger.info(f"å¹³å‡é€Ÿåº¦: {merge_result['total_records']/total_duration:.2f} è®°å½•/ç§’")
        logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        logger.info("ğŸ† å¤šè½®æ•°æ®ç”Ÿæˆä»»åŠ¡å®Œæˆ!")


def test_services_connection(services: List[str]) -> List[str]:
    """æµ‹è¯•æ‰€æœ‰æœåŠ¡çš„è¿æ¥"""
    import requests
    
    logger.info("ğŸ”Œ æµ‹è¯•æœåŠ¡è¿æ¥...")
    working_services = []
    
    for i, service in enumerate(services):
        try:
            response = requests.get(f"{service}/models", timeout=10)
            if response.status_code == 200:
                models = response.json()
                logger.info(f"âœ… æœåŠ¡ {i+1} ({service}): è¿æ¥æˆåŠŸ")
                working_services.append(service)
            else:
                logger.error(f"âŒ æœåŠ¡ {i+1} ({service}): çŠ¶æ€ç  {response.status_code}")
        except Exception as e:
            logger.error(f"âŒ æœåŠ¡ {i+1} ({service}): è¿æ¥å¤±è´¥ - {e}")
    
    logger.info(f"å¯ç”¨æœåŠ¡: {len(working_services)}/{len(services)}")
    return working_services


async def main():
    # é»˜è®¤æœåŠ¡åˆ—è¡¨
    default_services = [
        "http://localhost:6466/v1",
        "http://localhost:6467/v1",
        "http://localhost:6468/v1",
        "http://localhost:6469/v1",
        "http://localhost:6470/v1",
        "http://localhost:6471/v1",
        "http://localhost:6472/v1",
        "http://localhost:6473/v1"
    ]
    
    parser = argparse.ArgumentParser(description='åˆ†å¸ƒå¼å¹¶è¡Œç”Ÿæˆå¯¹è¯æ•°æ®')
    parser.add_argument('--input-file', help='è¾“å…¥çš„æ ·æœ¬JSONLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', required=True, help='è¾“å‡ºçš„JSONLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--services', nargs='+', default=default_services, help='APIæœåŠ¡åœ°å€åˆ—è¡¨')
    parser.add_argument('--model', default='/data/models/Qwen3-32B', help='æ¨¡å‹åç§°')
    parser.add_argument('--batch-size', type=int, default=16, help='æ¯ä¸ªæœåŠ¡çš„æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--max-concurrent', type=int, default=16, help='æ¯ä¸ªæœåŠ¡çš„æœ€å¤§å¹¶å‘æ•°')
    parser.add_argument('--min-score', type=int, default=8, help='æœ€ä½è¯„åˆ†è¦æ±‚(0-10)')
    parser.add_argument('--task-type', default='entity_extraction', help='ä»»åŠ¡ç±»å‹')
    parser.add_argument('--variants-per-sample', type=int, default=3, help='æ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„å˜ä½“æ•°é‡')
    parser.add_argument('--data-rounds', type=int, default=10, help='æ•°æ®ä½¿ç”¨è½®æ¬¡')
    parser.add_argument('--test-only', action='store_true', help='ä»…æµ‹è¯•æœåŠ¡è¿æ¥')
    parser.add_argument('--retry-times', default=3, type=int, help='é‡è¯•æ¬¡æ•°')
    parser.add_argument('--special-prompt', default="", type=str, help='ç‰¹æ®Šä»»åŠ¡æç¤ºè¯')
    parser.add_argument('--directions', nargs='*', default=['ä¿¡ç”¨å¡å¹´è´¹', 'è‚¡ç¥¨çˆ†ä»“', 'åŸºé‡‘èµå›'],help='éœ€è¦æ„é€ çš„é¢˜æï¼Œå¯è¾“å…¥å¤šä¸ªï¼Œå¦‚ï¼š--directions ä¿¡ç”¨å¡å¹´è´¹ è‚¡ç¥¨çˆ†ä»“')

    
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not args.test_only and not os.path.exists(args.input_file):
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        return
    
    # æµ‹è¯•æœåŠ¡è¿æ¥
    working_services = test_services_connection(args.services)
    
    if not working_services:
        logger.error("æ²¡æœ‰å¯ç”¨çš„æœåŠ¡ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
        return
    
    if args.test_only:
        logger.info("æœåŠ¡è¿æ¥æµ‹è¯•å®Œæˆ")
        return
    
    # åˆ›å»ºåˆ†å¸ƒå¼æ•°æ®ç”Ÿæˆå™¨
    generator = PipelineDataGenerator(
        services=working_services,
        model=args.model
    )

    # directions = json.loads(args.directions)
    
    # å¼€å§‹ç”Ÿæˆæ•°æ®
    await generator.generate_data(
        input_file=args.input_file,
        output_file=args.output,
        batch_size=args.batch_size,
        max_concurrent=args.max_concurrent,
        min_score=args.min_score,
        task_type=args.task_type,
        variants_per_sample=args.variants_per_sample,
        sample_retry_times=3,  # é»˜è®¤æ ·æœ¬é‡è¯•3æ¬¡
        data_rounds=args.data_rounds,
        model=args.model,
        retry_times=args.retry_times,
        special_prompt=args.special_prompt,
        directions=args.directions
    )


if __name__ == "__main__":
    asyncio.run(main()) 