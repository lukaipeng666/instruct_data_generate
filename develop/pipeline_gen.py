#!/usr/bin/env python3
"""
åˆ†å¸ƒå¼æ•°æ®ç”Ÿæˆç®¡é“
å°†æ ·æœ¬æ•°æ®åˆ†é…ç»™å¤šä¸ªæœ¬åœ°æ¨¡å‹æœåŠ¡åŒæ—¶å¤„ç†ï¼Œæœ€å¤§åŒ–æ•°æ®ç”Ÿæˆé€Ÿåº¦
"""

import json
import os
import asyncio
import time
import yaml
import redis
from pathlib import Path
from typing import List, Dict, Any, Optional

# å¯¼å…¥æ–°çš„æ¨¡å—
from develop.single_gen import main_process_from_samples
from develop.file_reader import FileReader


class PipelineDataGenerator:
    def __init__(self, services: List[str], model: str = "/data/models/Qwen3-32B",
                 api_key: str = "", is_vllm: bool = True, use_proxy: bool = True,
                 top_p: float = 1.0, max_tokens: int = 8192, timeout: int = 600):
        """
        åˆå§‹åŒ–åˆ†å¸ƒå¼æ•°æ®ç”Ÿæˆå™¨
        
        Args:
            services: APIæœåŠ¡åœ°å€åˆ—è¡¨ï¼Œå¦‚ ["http://localhost:6466/v1", ...]
            model: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥ï¼ˆOpenAIæ ¼å¼éœ€è¦ï¼‰
            is_vllm: æ˜¯å¦ä½¿ç”¨vLLMæ ¼å¼
            use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
            top_p: top_på‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¶…æ—¶æ—¶é—´
        """
        self.services = services
        self.model = model
        self.service_count = len(services)
        
        # æ¨¡å‹è°ƒç”¨ç›¸å…³å‚æ•°
        self.api_key = api_key
        self.is_vllm = is_vllm
        self.use_proxy = use_proxy
        self.top_p = top_p
        self.max_tokens = max_tokens
        self.timeout = timeout
        
        # Redis å®¢æˆ·ç«¯ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._redis_client = None
    
    def get_redis_client(self) -> Optional[redis.Redis]:
        """è·å– Redis å®¢æˆ·ç«¯ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
        if self._redis_client is None:
            try:
                config = self._get_yaml_config()
                redis_config = config.get('redis_service', {})
                host = redis_config.get('host', 'localhost')
                port = redis_config.get('port', 6379)
                db = redis_config.get('db', 0)
                password = redis_config.get('password', None)
                
                self._redis_client = redis.Redis(
                    host=host,
                    port=port,
                    db=db,
                    password=password,
                    decode_responses=True
                )
                # æµ‹è¯•è¿æ¥
                self._redis_client.ping()
            except Exception as e:
                print(f"âš ï¸  Redis è¿æ¥å¤±è´¥: {e}ï¼Œä»»åŠ¡è¿›åº¦å°†ä¸ä¼šè®°å½•åˆ° Redis")
                self._redis_client = None
        return self._redis_client
    
    def _get_yaml_config(self) -> dict:
        """è¯»å– YAML é…ç½®æ–‡ä»¶"""
        config_path = Path(__file__).parent.parent / "config" / "config.yaml"
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f) or {}
        return {}
    
    def update_task_progress(self, task_id: str, progress_data: dict):
        """
        æ›´æ–°ä»»åŠ¡è¿›åº¦åˆ° Redis
        
        Args:
            task_id: ä»»åŠ¡ID
            progress_data: è¿›åº¦æ•°æ®å­—å…¸
        """
        redis_client = self.get_redis_client()
        if redis_client:
            try:
                redis_key = f"task_progress:{task_id}"
                # æ›´æ–°è¿›åº¦æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
                redis_client.set(redis_key, json.dumps(progress_data, ensure_ascii=False))
                # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆ24å°æ—¶ï¼‰
                redis_client.expire(redis_key, 86400)
            except Exception as e:
                print(f"âš ï¸  Redis æ›´æ–°è¿›åº¦å¤±è´¥: {e}")
        
    def split_samples_in_memory(self, samples: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
        """
        åœ¨å†…å­˜ä¸­å°†æ ·æœ¬åˆ†é…ç»™å„ä¸ªæœåŠ¡ï¼ˆä¸å†™ä¸­é—´æ–‡ä»¶ï¼‰
        
        Args:
            samples: æ ·æœ¬æ•°æ®åˆ—è¡¨
            
        Returns:
            åˆ†é…åçš„æ ·æœ¬åˆ—è¡¨çš„åˆ—è¡¨
        """
        print(f"ğŸ“Š å†…å­˜ä¸­åˆ†é…æ ·æœ¬: æ€»æ•° {len(samples)}, æœåŠ¡æ•° {self.service_count}")
        
        # ä½¿ç”¨ FileReader çš„é™æ€æ–¹æ³•è¿›è¡Œåˆ†å‰²
        parts = FileReader.split_samples_in_memory(samples, self.service_count)
        
        for i, part in enumerate(parts):
            print(f"  æœåŠ¡ {i+1}: åˆ†é… {len(part)} ä¸ªæ ·æœ¬")
        
        return parts
    
    async def process_single_service(self, service_idx: int, api_base: str, 
                                   samples: List[Dict[str, Any]],
                                   task_id: str, user_id: int,
                                   batch_size: int = 5, 
                                   max_concurrent: int = 5, min_score: int = 8,
                                   task_type: str = "entity_extraction",
                                   variants_per_sample: int = 3,
                                   sample_retry_times: int = 3,
                                   model: str = "/data/models/Qwen3-32B",
                                   retry_times: int = 3,
                                   special_prompt: str = "",
                                   directions: list = ["ä¿¡ç”¨å¡å¹´è´¹"],
                                   api_key: str = "",
                                   is_vllm: bool = True,
                                   use_proxy: bool = False,
                                   top_p: float = 1.0,
                                   max_tokens: int = 8192,
                                   timeout: int = 600) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæœåŠ¡çš„ä»»åŠ¡ï¼Œæ•°æ®ç›´æ¥ä¿å­˜åˆ°SQLæ•°æ®åº“
        
        Args:
            service_idx: æœåŠ¡ç´¢å¼•
            api_base: APIåœ°å€
            samples: æ ·æœ¬æ•°æ®åˆ—è¡¨
            task_id: ä»»åŠ¡ID
            user_id: ç”¨æˆ·ID
            ...å…¶ä»–å‚æ•°...
        """
        
        print(f"ğŸš€ æœåŠ¡ {service_idx + 1} å¼€å§‹ç”Ÿæˆæ•°æ®: {api_base} (æ ·æœ¬æ•°: {len(samples)})")
        start_time = time.time()

        try:
            # ç›´æ¥ä¼ é€’æ ·æœ¬æ•°æ®ï¼Œä¿å­˜åˆ°æ•°æ®åº“
            result = await main_process_from_samples(
                samples=samples,
                task_id=task_id,
                user_id=user_id,
                api_base=api_base,
                model=model,
                batch_size=batch_size,
                max_concurrent=max_concurrent,
                retry_times=retry_times,
                min_score=min_score,
                task_type=task_type,
                variants_per_sample=variants_per_sample,
                sample_retry_times=sample_retry_times,
                special_prompt=special_prompt,
                directions=directions,
                api_key=api_key,
                is_vllm=is_vllm,
                use_proxy=use_proxy,
                top_p=top_p,
                max_tokens=max_tokens,
                timeout=timeout
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            # æ£€æŸ¥ç»“æœ
            if result.get("status") == "Success":
                output_count = result.get('output_count', 0)
                print(f"âœ… æœåŠ¡ {service_idx + 1} å®Œæˆ! è€—æ—¶: {duration:.1f}ç§’, ç”Ÿæˆæ•°æ®: {output_count}æ¡")
                
                return {
                    'service_idx': service_idx,
                    'api_base': api_base,
                    'success': True,
                    'start_time': start_time,
                    'end_time': end_time,
                    'duration': duration,
                    'input_samples': len(samples),
                    'output_count': output_count,
                    'stats': result.get('stats', {})
                }
            else:
                error_msg = result.get('error', 'unknown error')
                print(f"âŒ æœåŠ¡ {service_idx + 1} å¤„ç†å¤±è´¥: {error_msg}")
                return {
                    'service_idx': service_idx, 
                    'api_base': api_base,
                    'success': False, 
                    'error': error_msg,
                    'input_samples': len(samples),
                    'output_count': 0
                }
                
        except Exception as e:
            print(f"âŒ æœåŠ¡ {service_idx + 1} å¼‚å¸¸: {e}")
            return {
                'service_idx': service_idx, 
                'api_base': api_base,
                'success': False, 
                'error': str(e),
                'input_samples': len(samples),
                'output_count': 0
            }
    
    
    async def generate_data(self, task_id: str, user_id: int,
                          batch_size: int = 5, max_concurrent: int = 5,
                          min_score: int = 8, task_type: str = "entity_extraction",
                          variants_per_sample: int = 3, sample_retry_times: int = 3,
                          data_rounds: int = 3, model: str = "/data/models/Qwen3-32B",
                          retry_times: int = 3, special_prompt: str = "",
                          directions: list = ["ä¿¡ç”¨å¡å¹´è´¹"],
                          api_key: str = "", is_vllm: bool = True, use_proxy: bool = False,
                          top_p: float = 1.0, max_tokens: int = 8192, timeout: int = 600,
                          file_id: int = None):
        """
        ç”Ÿæˆæ•°æ®ï¼Œä½¿ç”¨å¤šä¸ªæœåŠ¡å¹¶è¡Œå¤„ç†ï¼Œæ”¯æŒå¤šè½®æ•°æ®ä½¿ç”¨
        æ•°æ®ç›´æ¥ä¿å­˜åˆ°SQLæ•°æ®åº“
        
        Args:
            task_id: ä»»åŠ¡IDï¼ˆå¿…éœ€ï¼‰
            user_id: ç”¨æˆ·IDï¼ˆå¿…éœ€ï¼‰
            file_id: è¾“å…¥æ–‡ä»¶IDï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨taskå…³è”çš„æ–‡ä»¶ï¼‰
            å…¶ä»–å‚æ•°: ç”Ÿæˆé…ç½®å‚æ•°
            
        Returns:
            åŒ…å«ä»»åŠ¡çŠ¶æ€ã€ç”Ÿæˆæ•°é‡ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        
        print("ğŸš€ å¼€å§‹åˆ†å¸ƒå¼æ•°æ®ç”Ÿæˆ")
        print(f"ä½¿ç”¨ {self.service_count} ä¸ªæœåŠ¡:")
        for i, service in enumerate(self.services):
            print(f"  æœåŠ¡ {i+1}: {service}")
        print(f"æ•°æ®ä½¿ç”¨è½®æ¬¡: {data_rounds} è½®")
        
        total_start_time = time.time()
        
        # 1. ä»æ•°æ®åº“è¯»å–è¾“å…¥æ•°æ®ï¼ˆä¸€æ¬¡æ€§è¯»å…¥å†…å­˜ï¼‰
        print(f"\nğŸ“‚ ä»æ•°æ®åº“è¯»å–æ•°æ® (file_id={file_id}, user_id={user_id})")
        samples, read_errors = FileReader.read_samples(file_id=file_id, user_id=user_id)
        
        if read_errors:
            print(f"âš ï¸ è¯»å–æ—¶æœ‰ {len(read_errors)} ä¸ªé”™è¯¯")
        print(f"âœ… è¯»å–å®Œæˆ: {len(samples)} ä¸ªæ ·æœ¬")
        
        if not samples:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼Œé€€å‡º")
            return {
                'status': 'Failed',
                'error': 'No valid samples',
                'total_generated': 0
            }
        
        # 2. å­˜å‚¨æ‰€æœ‰è½®æ¬¡çš„ç»“æœç»Ÿè®¡
        total_generated_count = 0
        
        # åˆå§‹åŒ–ä»»åŠ¡è¿›åº¦
        self.update_task_progress(task_id, {
            'task_id': task_id,
            'status': 'running',
            'current_round': 0,
            'total_rounds': data_rounds,
            'total_samples': len(samples),
            'generated_count': 0,
            'start_time': total_start_time,
            'services': self.service_count
        })
        
        # 3. å¤šè½®æ•°æ®å¤„ç†
        for round_num in range(data_rounds):
            print(f"\nğŸ”„ ç¬¬ {round_num + 1}/{data_rounds} è½®æ•°æ®ç”Ÿæˆ")
            
            # æ›´æ–° Redis è¿›åº¦ï¼šå½“å‰è½®æ¬¡å¼€å§‹
            self.update_task_progress(task_id, {
                'task_id': task_id,
                'status': 'running',
                'current_round': round_num + 1,
                'total_rounds': data_rounds,
                'total_samples': len(samples),
                'generated_count': total_generated_count,
                'start_time': total_start_time,
                'services': self.service_count,
                'round_status': 'processing'
            })
            
            # 3.1 åœ¨å†…å­˜ä¸­åˆ†é…æ ·æœ¬ç»™å„ä¸ªæœåŠ¡
            sample_parts = self.split_samples_in_memory(samples)
            
            # 3.2 åˆ›å»ºå¹¶è¡Œä»»åŠ¡
            print(f"âš¡ å¹¶è¡Œç”Ÿæˆ ({self.service_count} ä¸ªæœåŠ¡)")
            tasks = []
            
            for i, (service, sample_part) in enumerate(zip(self.services, sample_parts)):
                if not sample_part:
                    print(f"  æœåŠ¡ {i+1}: æ²¡æœ‰åˆ†é…åˆ°æ ·æœ¬ï¼Œè·³è¿‡")
                    continue
                
                task = self.process_single_service(
                    service_idx=i,
                    api_base=service,
                    samples=sample_part,
                    task_id=task_id,
                    user_id=user_id,
                    batch_size=batch_size,
                    max_concurrent=max_concurrent,
                    min_score=min_score,
                    task_type=task_type,
                    variants_per_sample=variants_per_sample,
                    sample_retry_times=sample_retry_times,
                    model=model,
                    retry_times=retry_times,
                    special_prompt=special_prompt,
                    directions=directions,
                    api_key=api_key if api_key else self.api_key,
                    is_vllm=is_vllm if is_vllm is not None else self.is_vllm,
                    use_proxy=use_proxy if use_proxy is not None else self.use_proxy,
                    top_p=top_p if top_p else self.top_p,
                    max_tokens=max_tokens if max_tokens else self.max_tokens,
                    timeout=timeout if timeout else self.timeout
                )
                tasks.append(task)
            
            # 3.3 ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # 3.4 ç»Ÿè®¡æœ¬è½®ç»“æœ
            print("ğŸ“ˆ æœ¬è½®ç»“æœç»Ÿè®¡")
            round_output_count = 0
            round_errors = 0
            
            for result in results:
                if isinstance(result, dict):
                    if result.get('success'):
                        round_output_count += result.get('output_count', 0)
                    else:
                        round_errors += 1
                elif isinstance(result, Exception):
                    print(f"âŒ ä»»åŠ¡å¼‚å¸¸: {result}")
                    round_errors += 1
            
            total_generated_count += round_output_count
            
            # æ›´æ–° Redis è¿›åº¦ï¼šå½“å‰è½®æ¬¡å®Œæˆ
            round_completion = ((round_num + 1) / data_rounds) * 100
            self.update_task_progress(task_id, {
                'task_id': task_id,
                'status': 'running',
                'current_round': round_num + 1,
                'total_rounds': data_rounds,
                'total_samples': len(samples),
                'generated_count': total_generated_count,
                'start_time': total_start_time,
                'services': self.service_count,
                'round_status': 'completed',
                'round_output': round_output_count,
                'round_errors': round_errors,
                'completion_percent': round(round_completion, 2)  # å®Œæˆç™¾åˆ†æ¯”
            })
            
            print(f"ç¬¬ {round_num + 1} è½®å®Œæˆ: ç”Ÿæˆ {round_output_count} æ¡æ•°æ®")
        
        # 4. è®¡ç®—æ€»è€—æ—¶
        total_duration = time.time() - total_start_time
        
        # è®¡ç®—å®Œæˆç™¾åˆ†æ¯”ï¼ˆ100%è¡¨ç¤ºå·²å®Œæˆï¼‰
        completion_percent = 100.0
        
        # æ›´æ–° Redis è¿›åº¦ï¼šä»»åŠ¡å®Œæˆ
        self.update_task_progress(task_id, {
            'task_id': task_id,
            'status': 'completed',
            'current_round': data_rounds,
            'total_rounds': data_rounds,
            'total_samples': len(samples),
            'generated_count': total_generated_count,
            'start_time': total_start_time,
            'end_time': time.time(),
            'duration': total_duration,
            'services': self.service_count,
            'completion_percent': completion_percent  # å®Œæˆç™¾åˆ†æ¯”
        })
        
        print(f"\nğŸ† å¤šè½®æ•°æ®ç”Ÿæˆä»»åŠ¡å®Œæˆ!")
        print(f"  æ€»è€—æ—¶: {total_duration:.1f}ç§’")
        print(f"  æ€»ç”Ÿæˆæ•°æ®: {total_generated_count} æ¡")
        
        return {
            'status': 'Success',
            'task_id': task_id,
            'total_generated': total_generated_count,
            'total_rounds': data_rounds,
            'total_duration': total_duration,
            'completion_percent': completion_percent  # æ·»åŠ å®Œæˆç™¾åˆ†æ¯”åˆ°è¿”å›å€¼
        }
