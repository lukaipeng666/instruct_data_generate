#!/usr/bin/env python3
"""
æ•°æ®ç”Ÿæˆè„šæœ¬
è¯»å–æ ·æœ¬æ•°æ®ï¼Œä½¿ç”¨æœ¬åœ°å¤§æ¨¡å‹ç”Ÿæˆæ–°æ•°æ®ï¼Œè¿›è¡Œè¯„ä¼°å¹¶ä¿å­˜åˆæ ¼çš„æ•°æ®
"""

import json
import traceback
import os
import asyncio
import time
import re
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import sys
import random
import threading
from threading import Lock

# çº¿ç¨‹å±€éƒ¨éšæœºæ•°ç”Ÿæˆå™¨ï¼Œé¿å…å¤šçº¿ç¨‹å…±äº«å…¨å±€éšæœºçŠ¶æ€
_thread_local = threading.local()


def _get_thread_random() -> random.Random:
    """
    è·å–çº¿ç¨‹å±€éƒ¨çš„éšæœºæ•°ç”Ÿæˆå™¨
    æ¯ä¸ªçº¿ç¨‹æ‹¥æœ‰ç‹¬ç«‹çš„ Random å®ä¾‹ï¼Œé¿å…å¤šçº¿ç¨‹ç«äº‰
    """
    if not hasattr(_thread_local, 'rng'):
        _thread_local.rng = random.Random()
    return _thread_local.rng

# å¯¼å…¥å·¥å…·å‡½æ•°
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.tools import (
    get_prompt_builder,
    get_format_evaluator
)
from config import get_default_services, get_default_model
# å¯¼å…¥æ¨¡å‹è°ƒç”¨å‡½æ•°
from call_model.model_call import call_model_api

# ä»é…ç½®è·å–é»˜è®¤å€¼
_default_api_base = get_default_services()[0] if get_default_services() else "http://localhost:16466/v1"
_default_model = get_default_model()


class DataGenerator:
    def __init__(self, 
                 api_base: str = None,
                 model: str = None,
                 max_concurrent: int = 5,
                 retry_times: int = 3,
                 min_score: int = 9,
                 task_type: str = "entity_extraction",
                 variants_per_sample: int = 3,
                 sample_retry_times: int = 3,
                 special_prompt: str = "",
                 directions: list = ["ä¿¡ç”¨å¡å¹´è´¹"],
                 api_key: str = "",
                 is_vllm: bool = True,
                 use_proxy: bool = False,
                 top_p: float = 1.0,
                 max_tokens: int = 8192,
                 timeout: int = 600):
        self.api_base = api_base or _default_api_base
        self.model = model or _default_model
        self.max_concurrent = max_concurrent
        self.retry_times = retry_times  # APIè°ƒç”¨é‡è¯•æ¬¡æ•°
        self.sample_retry_times = sample_retry_times  # æ ·æœ¬å¤„ç†é‡è¯•æ¬¡æ•°
        self.min_score = min_score  # æœ€ä½åˆ†æ•°è¦æ±‚
        self.task_type = task_type
        self.variants_per_sample = variants_per_sample
        
        # æ¨¡å‹è°ƒç”¨ç›¸å…³å‚æ•°
        self.api_key = api_key
        self.is_vllm = is_vllm
        self.use_proxy = use_proxy
        self.top_p = top_p
        self.max_tokens = max_tokens
        self.timeout = timeout
        
        # ä½¿ç”¨é”ä¿æŠ¤ç»Ÿè®¡æ•°æ®ï¼Œç¡®ä¿å¤šçº¿ç¨‹å®‰å…¨
        self._stats_lock = Lock()
        self.stats = {
            'samples_read': 0,
            'data_generated': 0,
            'data_evaluated': 0,
            'data_passed': 0,
            'data_failed': 0,
            'api_errors': 0,
            'sample_retries': 0  # æ–°å¢ï¼šæ ·æœ¬é‡è¯•æ¬¡æ•°ç»Ÿè®¡
        }
        
        # è·å–å¯é…ç½®çš„å‡½æ•°
        self.generation_prompt_builder = get_prompt_builder('generation')
        self.evaluation_prompt_builder = get_prompt_builder('evaluation')
        self.format_evaluator = get_format_evaluator(task_type)
        self.filter_prompt = get_prompt_builder('filter')
        self.special_prompt = special_prompt
        self.directions = directions
    
    async def init_session(self):
        """åˆå§‹åŒ–ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        pass
    
    async def close_session(self):
        """å…³é—­ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        pass
    
    async def call_api(self, prompt: str, temperature: float = 0.6) -> Optional[str]:
        """è°ƒç”¨æ¨¡å‹APIï¼ˆä½¿ç”¨ call_model æ¨¡å—ï¼‰"""
        messages = [
            {
                "role": "user",
                "content": prompt
            }
        ]
        
        try:
            # ä½¿ç”¨ call_model_api è¿›è¡Œè°ƒç”¨ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥å‡½æ•°ï¼‰
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: call_model_api(
                    api_url=self.api_base,
                    api_key=self.api_key,
                    messages=messages,
                    model=self.model,
                    temperature=temperature,
                    max_tokens=self.max_tokens,
                    retry_times=self.retry_times,
                    timeout=self.timeout,
                    is_vllm=self.is_vllm,
                    top_p=self.top_p,
                    use_proxy=self.use_proxy
                )
            )
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºé”™è¯¯å“åº”
            if response and (response.startswith("æ¨¡å‹è°ƒç”¨å¤±è´¥") or 
                            response.startswith("API Connection Error") or
                            response.startswith("Rate Limit Error") or
                            response.startswith("ä»£ç†è°ƒç”¨å¤±è´¥")):
                print(f"APIè°ƒç”¨å¤±è´¥: {response}")
                with self._stats_lock:
                    self.stats['api_errors'] += 1
                return None
            
            return response.strip() if response else None
            
        except Exception as e:
            print(f"APIè°ƒç”¨å¼‚å¸¸: {type(e).__name__}: {str(e)}")
            print(f"è¯¦ç»†å †æ ˆ: {traceback.format_exc()}")
            with self._stats_lock:
                self.stats['api_errors'] += 1
            return None
    
    def parse_generated_data(self, response: str, batch_idx: int = None, thread_idx: int = None, is_main_batch: bool = False, is_main_thread: bool = False) -> List[Dict[str, Any]]:
        """è§£æç”Ÿæˆçš„æ•°æ®
        
        Args:
            response: æ¨¡å‹è¾“å‡ºçš„åŸå§‹å“åº”
            batch_idx: æ‰¹æ¬¡ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
            thread_idx: çº¿ç¨‹/æ ·æœ¬ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
            is_main_batch: æ˜¯å¦ä¸ºä¸»æ‰¹æ¬¡ï¼ˆç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼‰
            is_main_thread: æ˜¯å¦ä¸ºä¸»çº¿ç¨‹ï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
        """
        # å°è¯•æå–JSONå†…å®¹
        try:
            # æ–¹æ³•1: æŸ¥æ‰¾JSONä»£ç å—
            # æ³¨æ„ï¼šä½¿ç”¨è´ªå©ªåŒ¹é…(.*)è€Œä¸æ˜¯éè´ªå©ªåŒ¹é…(.*?)
            # å› ä¸ºæ¨¡å‹è¾“å‡ºçš„textå­—æ®µä¸­å¯èƒ½åŒ…å«```ç¬¦å·ï¼Œéè´ªå©ªåŒ¹é…ä¼šè¿‡æ—©ç»“æŸ
            json_pattern = r'```json\s*(.*)\s*```'
            json_match = re.search(json_pattern, response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1).strip()
                try:
                    data = json.loads(json_str)
                    if isinstance(data, list):
                        return data
                    elif isinstance(data, dict):
                        return [data]
                except json.JSONDecodeError as e:
                    print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            
            # æ–¹æ³•2: å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
            try:
                data = json.loads(response.strip())
                if isinstance(data, list):
                    return data
                elif isinstance(data, dict):
                    return [data]
            except json.JSONDecodeError:
                pass
            
            # æ–¹æ³•3: æŸ¥æ‰¾ä»¥[å¼€å¤´]ç»“å°¾çš„æ•°ç»„
            # ä½¿ç”¨è´ªå©ªåŒ¹é…(.*)åŒ¹é…åˆ°æœ€åä¸€ä¸ª]ï¼Œé¿å…åµŒå¥—æ•°ç»„åŒ¹é…é”™è¯¯
            array_pattern = r'\[.*\]'
            array_match = re.search(array_pattern, response, re.DOTALL)
            if array_match:
                array_str = array_match.group(0)
                try:
                    data = json.loads(array_str)
                    if isinstance(data, list):
                        return data
                except json.JSONDecodeError as e:
                    print(f"âŒ æ•°ç»„æ¨¡å¼JSONè§£æå¤±è´¥: {e}")
            
            # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONå†…å®¹")
            return []
            
        except Exception as e:
            print(f"âŒ è§£æè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {type(e).__name__}: {e}")
            return []

    def parse_evaluation_score(self, response: str) -> Optional[int]:
        """è§£æè¯„ä¼°åˆ†æ•°"""
        import re
        
        # ä¼˜å…ˆæŸ¥æ‰¾\\boxed{}æ ¼å¼çš„è¯„åˆ†
        boxed_pattern = r'\\boxed\{(\d+)\}'
        boxed_match = re.search(boxed_pattern, response)
        if boxed_match:
            score = int(boxed_match.group(1))
            if 0 <= score <= 10:
                return score
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾æœ€åä¸€è¡Œçš„æ•°å­—è¯„åˆ†
        lines = response.strip().split('\n')
        for line in reversed(lines):
            line = line.strip()
            if line.isdigit() and 0 <= int(line) <= 10:
                return int(line)
        
        return None
    
    async def generate_data_from_sample(self, sample_data: Dict[str, Any], batch_idx: int = None, thread_idx: int = None, is_main_batch: bool = False, is_main_thread: bool = False) -> List[Dict[str, Any]]:
        """æ ¹æ®æ ·æœ¬æ•°æ®ç”Ÿæˆæ–°æ•°æ®
        
        Args:
            sample_data: æ ·æœ¬æ•°æ®
            batch_idx: æ‰¹æ¬¡ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
            thread_idx: çº¿ç¨‹/æ ·æœ¬ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
            is_main_batch: æ˜¯å¦ä¸ºä¸»æ‰¹æ¬¡ï¼ˆç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼‰
            is_main_thread: æ˜¯å¦ä¸ºä¸»çº¿ç¨‹ï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
        """
        try:
            # æ„å»ºç”Ÿæˆæç¤º
            if self.task_type == "calculation":
                # ä½¿ç”¨çº¿ç¨‹å±€éƒ¨éšæœºæ•°ç”Ÿæˆå™¨
                thread_rng = _get_thread_random()
                if self.directions == "éªŒè¯ç ":
                    # éšæœºç”Ÿæˆ4ä½æˆ–6ä½æ•°å­—éªŒè¯ç 
                    length = thread_rng.choice([4, 6])
                    verification_code = ''.join(str(thread_rng.randint(0, 9)) for _ in range(length))
                    result = [f"éšæœºç”Ÿæˆçš„{length}ä½éªŒè¯ç ï¼š{verification_code}"]
                elif self.directions == "æ‰‹æœºå·ç ":
                    # éšæœºç”Ÿæˆ11ä½ä¸­å›½å¤§é™†æ‰‹æœºå·ï¼ˆé¦–ä½å›ºå®šä¸º1ï¼Œç¬¬äºŒä½å¸¸è§ä¸º3/4/5/7/8ï¼‰
                    first = '1'
                    second = thread_rng.choice(['3', '4', '5', '7', '8'])
                    rest = ''.join(str(thread_rng.randint(0, 9)) for _ in range(9))
                    phone_number = first + second + rest
                    result = [f"éšæœºç”Ÿæˆçš„11ä½æ‰‹æœºå·ç ï¼š{phone_number}"]
                elif self.directions == "èº«ä»½è¯å·ç ":
                    # éšæœºç”Ÿæˆ18ä½èº«ä»½è¯å·ï¼ˆå‰6ä½åœ°å€ç ç®€åŒ–å¤„ç†ï¼Œç¬¬7-14ä½ç”Ÿæ—¥éšæœºï¼Œæœ€å1ä½å¯èƒ½ä¸ºXï¼‰
                    address_code = ''.join(str(thread_rng.randint(0, 9)) for _ in range(6))  # ç®€åŒ–åœ°å€ç 
                    year = str(thread_rng.randint(1950, 2005))  # éšæœºå¹´ä»½
                    month = f"{thread_rng.randint(1, 12):02d}"  # æœˆä»½è¡¥0
                    day = f"{thread_rng.randint(1, 28):02d}"  # æ—¥æœŸç®€åŒ–å¤„ç†ï¼ˆ1-28ï¼‰
                    birth_code = year + month + day
                    seq_code = ''.join(str(thread_rng.randint(0, 9)) for _ in range(3))  # é¡ºåºç 
                    last_code = thread_rng.choice([str(i) for i in range(10)] + ['X'])  # æ ¡éªŒç ï¼ˆå¯èƒ½ä¸ºXï¼‰
                    id_card = address_code + birth_code + seq_code + last_code
                    result = [f"éšæœºç”Ÿæˆçš„18ä½èº«ä»½è¯å·ç ï¼š{id_card}"]
                else:
                    num_length = thread_rng.randint(4, 35)
                    num_str = str(thread_rng.randint(1000, 10**num_length))
                    result = [f"éšæœºç”Ÿæˆçš„é•¿åº¦ä¸º{num_length}çš„æ•°å­—{num_str}"]
            else:
                result = self.directions
            prompt = self.generation_prompt_builder(sample_data, self.variants_per_sample, self.special_prompt, result)
        
            # è°ƒç”¨APIç”Ÿæˆæ•°æ®
            response = await self.call_api(prompt, temperature=0.3)
            if response is None:
                print("âŒ ç”Ÿæˆæ•°æ®APIè°ƒç”¨å¤±è´¥")
                return []
            
            # è§£æç”Ÿæˆçš„æ•°æ®
            generated_list = self.parse_generated_data(response, batch_idx, thread_idx, is_main_batch, is_main_thread)
            with self._stats_lock:
                self.stats['data_generated'] += len(generated_list)
            return generated_list
            
        except Exception as e:
            print(f"ç”Ÿæˆæ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return []
    
    async def evaluate_generated_data(self, sample_data: Dict[str, Any], generated_data: Dict[str, Any]) -> Tuple[int, int]:
        """è¯„ä¼°ç”Ÿæˆçš„æ•°æ®ï¼Œè¿”å›(æ¨¡å‹è¯„åˆ†, è§„åˆ™è¯„åˆ†)"""
        try:
            # è·å–Assistantçš„å›ç­”ç”¨äºè§„åˆ™è¯„ä¼°
            assistant_text = ""
            # ä¿®å¤ï¼šturnsåº”è¯¥æ˜¯åˆ—è¡¨ï¼Œé»˜è®¤å€¼åº”è¯¥æ˜¯[]è€Œä¸æ˜¯{}
            turns = generated_data.get('turns', [])
            
            if not isinstance(turns, list):
                print(f"âŒ turnsä¸æ˜¯åˆ—è¡¨ç±»å‹ï¼Œè·³è¿‡è¯¥æ•°æ®")
                return 0, 0
            
            for turn in turns:
                if not isinstance(turn, dict):
                    continue
                if turn.get('role') == 'Assistant':
                    assistant_text = turn.get('text', '')
                    break
            
            Assistant = 0
            Human = 0
            for turn in turns:
                if not isinstance(turn, dict):
                    continue
                role = turn.get('role', '')
                # å¤„ç†roleå­—æ®µï¼šå»é™¤é¦–å°¾ç©ºæ ¼
                if isinstance(role, str):
                    role = role.strip()
                if role == 'Assistant':
                    Assistant += 1
                elif role == 'Human':
                    Human += 1
            # è§„åˆ™è¯„åˆ†
            rule_score = 0
            model_score = 0
            if Assistant == 1 and Human == 1:
                rule_score = self.format_evaluator(assistant_text)
                if rule_score == 10:
                    # æ¨¡å‹è¯„åˆ†
                    eval_prompt = self.evaluation_prompt_builder(sample_data, generated_data, self.special_prompt)
                    eval_response_list = []
                    for _ in range(1):
                        eval_response = await self.call_api(eval_prompt, temperature=0.2)
                        eval_response_list.append(eval_response)
                        model_score_ = self.parse_evaluation_score(eval_response)
                        if (model_score_ and model_score_ < self.min_score) or not model_score_:
                            return 0, 0
                    if all(eval_response for eval_response in eval_response_list):
                        model_score_list = [self.parse_evaluation_score(eval_response) for eval_response in eval_response_list]
                        if all((model_score and model_score >= self.min_score) for model_score in model_score_list):
                            model_score = sum(model_score_list) / len(model_score_list)
                        else:
                            model_score = 0
                        if model_score is None:
                            model_score = 0
            with self._stats_lock:
                self.stats['data_evaluated'] += 1
            return model_score, rule_score
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return 0, 0
    

    async def evaluate_data(self, content: str) -> int:
        """è¯„ä¼°ç”Ÿæˆçš„æ•°æ®ï¼Œè¿”å›(æ¨¡å‹è¯„åˆ†, è§„åˆ™è¯„åˆ†)"""
        try:
            model_score = 0

            # æ¨¡å‹è¯„åˆ†
            eval_prompt = self.filter_prompt(content)
            eval_response_list = []
            for _ in range(1):
                eval_response = await self.call_api(eval_prompt, temperature=0.2)
                eval_response_list.append(eval_response)
                model_score_ = self.parse_evaluation_score(eval_response)
                if (model_score_ and model_score_ < self.min_score) or not model_score_:
                    return model_score_, eval_response
            if all(eval_response for eval_response in eval_response_list):
                model_score_list = [self.parse_evaluation_score(eval_response) for eval_response in eval_response_list]
                if all((model_score and model_score >= self.min_score) for model_score in model_score_list):
                    model_score = sum(model_score_list) / len(model_score_list)
                else:
                    model_score = 0
                if model_score is None:
                    model_score = 0

            return model_score_, eval_response
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return 0, "è¯„åˆ†æ—¶å‡ºé”™"
    
    async def process_single_sample(self, sample_data: Dict[str, Any], batch_idx: int = None, thread_idx: int = None, is_main_batch: bool = False, is_main_thread: bool = False) -> List[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œç”Ÿæˆå¹¶è¯„ä¼°æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆæ ¼æ•°æ®åˆ™é‡è¯•
        
        Args:
            sample_data: æ ·æœ¬æ•°æ®
            batch_idx: æ‰¹æ¬¡ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
            thread_idx: çº¿ç¨‹/æ ·æœ¬ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
            is_main_batch: æ˜¯å¦ä¸ºä¸»æ‰¹æ¬¡ï¼ˆç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼‰
            is_main_thread: æ˜¯å¦ä¸ºä¸»çº¿ç¨‹ï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
        """
        for retry_count in range(self.sample_retry_times):
            try:
                # ç”Ÿæˆæ•°æ®
                generated_list = await self.generate_data_from_sample(sample_data, batch_idx, thread_idx, is_main_batch, is_main_thread)
                
                if not generated_list:
                    if retry_count < self.sample_retry_times - 1:
                        with self._stats_lock:
                            self.stats['sample_retries'] += 1
                        continue
                    else:
                        return []
                
                # è¯„ä¼°æ¯ä¸ªç”Ÿæˆçš„æ•°æ®
                qualified_data = []
                
                for idx, generated_data in enumerate(generated_list):
                    if not isinstance(generated_data, dict):
                        continue
                    if 'turns' not in generated_data:
                        continue
                    
                    model_score, rule_score = await self.evaluate_generated_data(sample_data, generated_data)
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€ä½åˆ†æ•°è¦æ±‚ï¼šè§„åˆ™è¯„åˆ†å¿…é¡»æ»¡åˆ†ï¼Œæ¨¡å‹è¯„åˆ†è¾¾åˆ°æœ€ä½è¦æ±‚
                    if model_score >= self.min_score and rule_score == 10:
                        # æ„å»ºå®Œæ•´çš„æ•°æ®ç»“æ„
                        complete_data = {
                            'meta': sample_data.get('meta', {}).copy(),
                            'turns': generated_data.get('turns', [])
                        }
                        
                        # æ·»åŠ ç”Ÿæˆå…ƒæ•°æ®
                        complete_data['meta']['generated'] = True
                        complete_data['meta']['generation_model'] = self.model.split('/')[-1]
                        complete_data['meta']['generation_time'] = datetime.now().isoformat()
                        complete_data['meta']['model_score'] = model_score
                        complete_data['meta']['rule_score'] = rule_score
                        complete_data['meta']['source_task'] = self.task_type
                        complete_data['meta']['retry_count'] = retry_count  # è®°å½•é‡è¯•æ¬¡æ•°
                        
                        qualified_data.append(complete_data)
                        with self._stats_lock:
                            self.stats['data_passed'] += 1
                    else:
                        with self._stats_lock:
                            self.stats['data_failed'] += 1
                
                # å¦‚æœæœ‰åˆæ ¼æ•°æ®ï¼Œç›´æ¥è¿”å›
                if qualified_data:
                    return qualified_data
                
                # å¦‚æœæ²¡æœ‰åˆæ ¼æ•°æ®ä¸”è¿˜å¯ä»¥é‡è¯•
                if retry_count < self.sample_retry_times - 1:
                    with self._stats_lock:
                        self.stats['sample_retries'] += 1
                    continue
                else:
                    return []
                
            except Exception as e:
                if retry_count < self.sample_retry_times - 1:
                    print(f"âŒ å¤„ç†æ ·æœ¬æ—¶å‡ºé”™: {str(e)}ï¼Œé‡è¯•ä¸­...")
                    with self._stats_lock:
                        self.stats['sample_retries'] += 1
                    continue
                else:
                    print(f"âŒ é‡è¯•{self.sample_retry_times}æ¬¡åä»ç„¶å‡ºé”™: {str(e)}")
                    return []
        
        return []
    
    async def process_batch(self, samples: List[Dict[str, Any]], batch_idx: int = None, is_main_batch: bool = False) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†æ ·æœ¬
        
        Args:
            samples: æ ·æœ¬åˆ—è¡¨
            batch_idx: æ‰¹æ¬¡ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
            is_main_batch: æ˜¯å¦ä¸ºä¸»æ‰¹æ¬¡ï¼ˆç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼‰
        """
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def process_with_semaphore(sample, thread_idx):
            async with semaphore:
                # åˆ¤æ–­æ˜¯å¦ä¸ºä¸»çº¿ç¨‹ï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
                is_main_thread = (thread_idx == 0)
                return await self.process_single_sample(sample, batch_idx, thread_idx, is_main_batch, is_main_thread)
        
        tasks = [process_with_semaphore(sample, idx) for idx, sample in enumerate(samples)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ”¶é›†æ‰€æœ‰åˆæ ¼çš„æ•°æ®
        all_qualified_data = []
        for result in results:
            if isinstance(result, list):
                all_qualified_data.extend(result)
            elif isinstance(result, Exception):
                print(f"æ‰¹å¤„ç†ä¸­å‡ºç°å¼‚å¸¸: {result}")
        
        return all_qualified_data
    
    async def generate_from_samples(self, samples: List[Dict[str, Any]], 
                                     batch_size: int = 5,
                                     task_id: str = None,
                                     user_id: int = None) -> Dict[str, Any]:
        """
        ä»å†…å­˜ä¸­çš„æ ·æœ¬æ•°æ®ç”Ÿæˆæ•°æ®å¹¶ä¿å­˜åˆ°æ•°æ®åº“
        
        Args:
            samples: æ ·æœ¬æ•°æ®åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            task_id: ä»»åŠ¡IDï¼ˆå¿…éœ€ï¼‰
            user_id: ç”¨æˆ·IDï¼ˆå¿…éœ€ï¼‰
            
        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯å’Œç”Ÿæˆç»“æœçš„å­—å…¸
        """
        print(f"ğŸ“Š å¼€å§‹å¤„ç† {len(samples)} ä¸ªæ ·æœ¬")
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if not task_id or not user_id:
            raise ValueError("ä¿å­˜åˆ°æ•°æ®åº“æ—¶å¿…é¡»æä¾› task_id å’Œ user_id")
        
        # é‡ç½®ç»Ÿè®¡æ•°æ®
        self.stats['samples_read'] = len(samples)
        
        # åˆå§‹åŒ–ä¼šè¯
        await self.init_session()
        
        try:
            # å¯¼å…¥æ•°æ®åº“æœåŠ¡
            sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            from database import save_batch_generated_data
            
            
            # åˆ†æ‰¹å¤„ç†
            all_qualified_data = []
            for i in range(0, len(samples), batch_size):
                batch = samples[i:i + batch_size]
                batch_idx = i // batch_size
                is_main_batch = (batch_idx == 0)  # ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ä¸ºä¸»æ‰¹æ¬¡
                print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{(len(samples) + batch_size - 1)//batch_size}")
                
                # å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_results = await self.process_batch(batch, batch_idx, is_main_batch)
                
                all_qualified_data.extend(batch_results)
                
                # æ‰¹é‡ä¿å­˜å½“å‰æ‰¹æ¬¡çš„æ•°æ®åˆ°æ•°æ®åº“
                if batch_results:
                    try:
                        saved_count = save_batch_generated_data(
                            task_id=task_id,
                            user_id=user_id,
                            data_list=batch_results,
                            generation_model=self.model.split('/')[-1],
                            task_type=self.task_type
                        )
                    except Exception as e:
                        print(f"âŒ ä¿å­˜æ‰¹æ¬¡ {batch_idx + 1} æ•°æ®å¤±è´¥: {e}")
                        raise e
                
            
            print(f"âœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼Œå…± {len(all_qualified_data)} æ¡åˆæ ¼æ•°æ®")
            
            # è¿”å›ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
            return {
                'status': 'Success',
                'output_count': len(all_qualified_data),
                'qualified_data': all_qualified_data,
                'stats': self.stats.copy(),
                'task_id': task_id
            }
        
        except Exception as e:
            print(f"å¤„ç†æ ·æœ¬æ—¶å‡ºé”™: {str(e)}")
            return {
                'status': 'Failed',
                'error': str(e),
                'output_count': 0,
                'stats': self.stats.copy()
            }
        
        finally:
            await self.close_session()

async def main_process_from_samples(samples: List[Dict[str, Any]], 
                                     api_base: str, model: str, batch_size: int, 
                                     max_concurrent: int, retry_times: int, min_score: int, 
                                     task_type: str, variants_per_sample: int, 
                                     sample_retry_times: int, special_prompt: str, 
                                     directions: list, 
                                     task_id: str,
                                     user_id: int,
                                     api_key: str = "", 
                                     is_vllm: bool = True, use_proxy: bool = False,
                                     top_p: float = 1.0, max_tokens: int = 8192, 
                                     timeout: int = 600) -> Dict[str, Any]:
    """
    ä¸»å¤„ç†å‡½æ•° - ç”Ÿæˆæ•°æ®å¹¶ä¿å­˜åˆ°SQLæ•°æ®åº“
    
    Args:
        samples: æ ·æœ¬æ•°æ®åˆ—è¡¨
        api_base: APIæœåŠ¡åœ°å€
        model: æ¨¡å‹åç§°
        batch_size: æ‰¹å¤„ç†å¤§å°
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        retry_times: é‡è¯•æ¬¡æ•°
        min_score: æœ€ä½è¯„åˆ†è¦æ±‚
        task_type: ä»»åŠ¡ç±»å‹
        variants_per_sample: æ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„å˜ä½“æ•°é‡
        sample_retry_times: æ ·æœ¬å¤„ç†é‡è¯•æ¬¡æ•°
        special_prompt: ç‰¹æ®Šæç¤ºè¯
        directions: ç”Ÿæˆæ–¹å‘åˆ—è¡¨
        task_id: ä»»åŠ¡IDï¼ˆå¿…éœ€ï¼‰
        user_id: ç”¨æˆ·IDï¼ˆå¿…éœ€ï¼‰
        api_key: APIå¯†é’¥
        is_vllm: æ˜¯å¦ä½¿ç”¨vLLMæ ¼å¼
        use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        top_p: top_på‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        timeout: è¶…æ—¶æ—¶é—´
        
    Returns:
        åŒ…å«ç»Ÿè®¡ä¿¡æ¯å’Œç”Ÿæˆç»“æœçš„å­—å…¸
    """
    try:
        if not samples:
            print("è¾“å…¥æ ·æœ¬ä¸ºç©º")
            return {"status": "Failed", "error": "è¾“å…¥æ ·æœ¬ä¸ºç©º", "output_count": 0}
        
        # åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨
        generator = DataGenerator(
            api_base=api_base,
            model=model,
            max_concurrent=max_concurrent,
            retry_times=retry_times,
            min_score=min_score,
            task_type=task_type,
            variants_per_sample=variants_per_sample,
            sample_retry_times=sample_retry_times,
            special_prompt=special_prompt,
            directions=directions,
            api_key=api_key,
            is_vllm=is_vllm,
            use_proxy=use_proxy,
            top_p=top_p,
            max_tokens=max_tokens,
            timeout=timeout
        )
        
        # å¼€å§‹ç”Ÿæˆæ•°æ®ï¼ˆç›´æ¥ä»å†…å­˜ä¸­çš„æ ·æœ¬ï¼‰
        start_time = time.time()
        result = await generator.generate_from_samples(
            samples=samples,
            batch_size=batch_size,
            task_id=task_id,
            user_id=user_id
        )
        end_time = time.time()
        
        print(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
        
        # è¿”å›å®Œæ•´ç»“æœ
        result['duration'] = end_time - start_time
        result['input_samples'] = len(samples)
        return result
        
    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}")
        return {"status": "Failed", "error": str(e), "output_count": 0}

# æ³¨æ„ï¼šæœ¬æ¨¡å—çš„ç‹¬ç«‹è¿è¡Œæ¨¡å¼å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ main.py å…¥å£è°ƒç”¨